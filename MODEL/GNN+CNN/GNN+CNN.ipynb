{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (2.1.3)\n",
            "Requirement already satisfied: pandas in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (2.19.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (3.10.1)\n",
            "Requirement already satisfied: seaborn in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: jupyterlab in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (4.4.1)\n",
            "Requirement already satisfied: torch in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (2.6.0)\n",
            "Requirement already satisfied: torch-geometric in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (2.6.1)\n",
            "Requirement already satisfied: rdkit in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (2024.9.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (2.0.5)\n",
            "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (0.28.1)\n",
            "Requirement already satisfied: ipykernel>=6.5.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (6.29.5)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (5.7.2)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (2.2.5)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (2.15.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (2.27.3)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (0.2.4)\n",
            "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (6.4.2)\n",
            "Requirement already satisfied: traitlets in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab) (5.14.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from torch-geometric) (3.11.18)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from torch-geometric) (7.0.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab) (0.16.0)\n",
            "Requirement already satisfied: comm>=0.1.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (1.8.14)\n",
            "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (9.2.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (8.6.3)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=24 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (26.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab) (3.0.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-core->jupyterlab) (4.3.7)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-core->jupyterlab) (310)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (23.1.0)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.5.3)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.16.6)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.21.1)\n",
            "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (2.0.15)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (4.23.0)\n",
            "Requirement already satisfied: rich in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from anyio->httpx>=0.25.0->jupyterlab) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (21.2.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.19.2)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (2.19.1)\n",
            "Requirement already satisfied: stack_data in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.6.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.24.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0.2)\n",
            "Requirement already satisfied: rfc3339-validator in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.13.4)\n",
            "Requirement already satisfied: bleach[css]!=5.0.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.8.4)\n",
            "Requirement already satisfied: fqdn in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (1.5.1)\n",
            "Requirement already satisfied: isoduration in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (3.0.0)\n",
            "Requirement already satisfied: uri-template in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (24.11.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.7)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.2.3)\n",
            "Requirement already satisfied: pycparser in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (2.22)\n",
            "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\nongnam\\documents\\ai_builder\\.venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (2.9.0.20241206)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas scikit-learn tensorflow matplotlib seaborn jupyterlab torch torch-geometric rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oWSzPzgfI-Ro"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# === Core Libraries ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import glob\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "\n",
        "# === RDKit for Molecular Processing ===\n",
        "from rdkit import Chem\n",
        "\n",
        "# === PyTorch & PyTorch Geometric ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, NNConv, global_mean_pool\n",
        "\n",
        "# === TensorFlow / Keras ===\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
        "    BatchNormalization, Dropout, Dense, Concatenate, Add, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "# === Scikit-learn ===\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "zbLxZIAghLDq",
        "outputId": "ecad2237-2429-4f1c-e1a2-3548ebe223d8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMILES</th>\n",
              "      <th>target_sequence</th>\n",
              "      <th>Kd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COc1cc2nc(cnc2cc1OCC#N)-c1ccc(CC(=O)Nc2cc(on2)...</td>\n",
              "      <td>MPALARDGGQLPLLVVFSAMIFGTITNQDLPVIKCVLINHKNNDSS...</td>\n",
              "      <td>8.301030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cn1nc(C(=O)NC[C@@]2(O)CC[C@@H](CC2)S(C)(=O)=O)...</td>\n",
              "      <td>MDRAPQRQHRASRELLAAKKTHTSQIEVIPCKICGDKSSGIHYGVI...</td>\n",
              "      <td>7.795880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Oc1ccc(/C=C/c2cc(O)cc(O)c2)cc1</td>\n",
              "      <td>MDDIYKAAVEQLTEEQKNEFKAAFDIFVLGAEDGCISTKELGKVMR...</td>\n",
              "      <td>3.614394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cc1ccc(CNc2nc3c(N)ncnc3n2[C@@H]2O[C@H](CO)[C@@...</td>\n",
              "      <td>MSKGPAVGIDLGTTYSCVGVFQHGKVEIIANDQGNRTTPSYVAFTD...</td>\n",
              "      <td>6.530178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC\\C=C\\CCOC(=O)c1cccnc1</td>\n",
              "      <td>MSALGVTVALLVWAAFLLLVSMWRQVHSSWNLPPGPFPLPIIGNLF...</td>\n",
              "      <td>5.886057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              SMILES  \\\n",
              "0  COc1cc2nc(cnc2cc1OCC#N)-c1ccc(CC(=O)Nc2cc(on2)...   \n",
              "1  Cn1nc(C(=O)NC[C@@]2(O)CC[C@@H](CC2)S(C)(=O)=O)...   \n",
              "2                     Oc1ccc(/C=C/c2cc(O)cc(O)c2)cc1   \n",
              "3  Cc1ccc(CNc2nc3c(N)ncnc3n2[C@@H]2O[C@H](CO)[C@@...   \n",
              "4                            CC\\C=C\\CCOC(=O)c1cccnc1   \n",
              "\n",
              "                                     target_sequence        Kd  \n",
              "0  MPALARDGGQLPLLVVFSAMIFGTITNQDLPVIKCVLINHKNNDSS...  8.301030  \n",
              "1  MDRAPQRQHRASRELLAAKKTHTSQIEVIPCKICGDKSSGIHYGVI...  7.795880  \n",
              "2  MDDIYKAAVEQLTEEQKNEFKAAFDIFVLGAEDGCISTKELGKVMR...  3.614394  \n",
              "3  MSKGPAVGIDLGTTYSCVGVFQHGKVEIIANDQGNRTTPSYVAFTD...  6.530178  \n",
              "4  MSALGVTVALLVWAAFLLLVSMWRQVHSSWNLPPGPFPLPIIGNLF...  5.886057  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Load data ===\n",
        "df = pd.read_csv(\"Kd.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "8CFafgX2iJBw",
        "outputId": "8fd9e798-95a5-4f76-b2d0-31bc9d6ef6a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SMILES             0\n",
              "target_sequence    0\n",
              "Kd                 0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAl2Ls55wSik",
        "outputId": "ef845a06-bf3e-402a-9060-cec655a80a60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "55011"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x_cMJtBF7rc3"
      },
      "outputs": [],
      "source": [
        "# Encode amino acid sequences\n",
        "aa_list = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "aa_to_int = {aa: i + 1 for i, aa in enumerate(aa_list)}\n",
        "\n",
        "def encode_protein(seq):\n",
        "    return [aa_to_int.get(aa, 0) for aa in seq]\n",
        "\n",
        "df['protein_encoded'] = df['target_sequence'].apply(encode_protein)\n",
        "\n",
        "# Pad protein sequences\n",
        "max_len_protein = 1000\n",
        "X_protein = pad_sequences(df['protein_encoded'], maxlen=max_len_protein, padding='post')\n",
        "\n",
        "# Store Kd values\n",
        "y_kd = df['Kd'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_atom_features(atom):\n",
        "    return [\n",
        "        atom.GetAtomicNum(),\n",
        "        atom.GetDegree(),\n",
        "        atom.GetFormalCharge(),\n",
        "        int(atom.GetHybridization()),\n",
        "        int(atom.GetIsAromatic())\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bond_features(bond):\n",
        "    return [\n",
        "        int(bond.GetBondTypeAsDouble()),   # Single=1.0, Double=2.0, etc.\n",
        "        int(bond.GetIsConjugated()),\n",
        "        int(bond.GetIsAromatic())\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uws0C2grFKsr",
        "outputId": "7435fe0a-8888-4e13-aba8-da69fd9f43b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[18:34:46] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
            "[18:34:51] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
            "[18:34:57] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
            "[18:35:07] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
            "[18:35:29] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
            "[18:35:41] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20 21 22 23 25\n",
            "[18:35:50] Explicit valence for atom # 22 N, 4, is greater than permitted\n"
          ]
        }
      ],
      "source": [
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "\n",
        "    smiles = Chem.MolToSmiles(mol)\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    x = torch.tensor([get_atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float)\n",
        "\n",
        "    edge_index = []\n",
        "    edge_attr = []\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "        edge_index += [[i, j], [j, i]]\n",
        "        bond_feat = get_bond_features(bond)\n",
        "        edge_attr += [bond_feat, bond_feat]  # Add both directions\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "# Apply and filter out None values (only valid SMILES)\n",
        "graph_data_list = []\n",
        "valid_indices = []\n",
        "\n",
        "for i, smiles in enumerate(df['SMILES']):\n",
        "    graph = smiles_to_graph(smiles)\n",
        "    if graph is not None:\n",
        "        graph_data_list.append(graph)\n",
        "        valid_indices.append(i)\n",
        "\n",
        "# Filter protein and Kd tensors to match valid molecules\n",
        "X_protein = X_protein[valid_indices]\n",
        "y_kd = y_kd[valid_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(x=[39, 5], edge_index=[2, 86], edge_attr=[86, 3])\n"
          ]
        }
      ],
      "source": [
        "print(graph_data_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bjsQ_rgFD6OL"
      },
      "outputs": [],
      "source": [
        "class GNNEncoder(nn.Module):\n",
        "    def __init__(self, node_in=5, edge_in=3):\n",
        "        super().__init__()\n",
        "        self.edge_nn1 = nn.Sequential(\n",
        "            nn.Linear(edge_in, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, node_in * 64)\n",
        "        )\n",
        "        self.conv1 = NNConv(node_in, 64, self.edge_nn1, aggr='mean')\n",
        "\n",
        "        self.edge_nn2 = nn.Sequential(\n",
        "            nn.Linear(edge_in, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64 * 128)\n",
        "        )\n",
        "        self.conv2 = NNConv(64, 128, self.edge_nn2, aggr='mean')\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "        x = torch.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = torch.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6y5pi6_9I3oK",
        "outputId": "86e42c4b-ed88-4cf8-c1b2-7d06df1702d1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ protein_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ protein_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ protein_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m1,344\u001b[0m │ protein_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m73,856\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m295,168\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m41,088\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m384\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m256\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">595,649</span> (2.27 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m595,649\u001b[0m (2.27 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">594,113</span> (2.27 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m594,113\u001b[0m (2.27 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Protein CNN Branch ---\n",
        "protein_input = Input(shape=(max_len_protein,), name=\"protein_input\")\n",
        "x = Embedding(input_dim=len(aa_to_int) + 1, output_dim=64)(protein_input)\n",
        "x1 = Conv1D(128, kernel_size=9, activation='relu', padding='same')(x)\n",
        "x1 = BatchNormalization()(x1)\n",
        "x1 = Conv1D(256, kernel_size=9, activation='relu', padding='same')(x1)\n",
        "x1 = BatchNormalization()(x1)\n",
        "\n",
        "x2 = Conv1D(128, kernel_size=5, activation='relu', padding='same')(x)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "# Skip connection via concatenation\n",
        "merged_cnn = Concatenate()([x1, x2])\n",
        "x_cnn = GlobalMaxPooling1D()(merged_cnn)\n",
        "x_cnn = Dropout(0.4)(x_cnn)\n",
        "\n",
        "# --- GNN Features Input ---\n",
        "gnn_input = Input(shape=(128,), name=\"gnn_input\")\n",
        "\n",
        "# Optional layer norm or transformation layer\n",
        "gnn_proj = Dense(128, activation='relu')(gnn_input)\n",
        "gnn_proj = LayerNormalization()(gnn_proj)\n",
        "\n",
        "# --- Merge branches ---\n",
        "merged = Concatenate()([x_cnn, gnn_proj])\n",
        "\n",
        "# Fully connected layers\n",
        "x = Dense(256, activation='relu')(merged)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "# x = Dense(64, activation='relu')(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n",
        "# Output layer\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "model = Model(inputs=[protein_input, gnn_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss=MeanSquaredError(), metrics=['mae'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EpochHistorySaver(Callback):\n",
        "    def __init__(self, save_dir='GNN_MODEL'):\n",
        "        super().__init__()\n",
        "        self.save_dir = save_dir\n",
        "        self.history = {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        for key, value in logs.items():\n",
        "            self.history.setdefault(key, []).append(value)\n",
        "\n",
        "        # Save history after each epoch\n",
        "        filename = f'{self.save_dir}/history_epoch_{epoch+1:02d}.pkl'\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.history, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y38LAo1A3gyx"
      },
      "outputs": [],
      "source": [
        "# Save the model at every epoch with the epoch number in the filename\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    filepath='GNN_MODEL/model_epoch_{epoch:02d}.h5',\n",
        "    save_freq='epoch',\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_cb = EpochHistorySaver(save_dir='GNN_MODEL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataLoader for batching the graph data\n",
        "batch_size = 32  # You can experiment with this size depending on your memory capacity\n",
        "\n",
        "loader = DataLoader(graph_data_list, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting GNN Features: 100%|██████████| 1719/1719 [03:39<00:00,  7.83it/s]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the GNN encoder model\n",
        "gnn_model = GNNEncoder()\n",
        "gnn_model.eval()\n",
        "gnn_model.cpu()  # Use CPU if GPU memory is limited\n",
        "\n",
        "# List to store the GNN features\n",
        "gnn_features_list = []\n",
        "\n",
        "# Iterate over the DataLoader with tqdm for progress tracking\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(loader, desc=\"Extracting GNN Features\"):\n",
        "        gnn_features = gnn_model(batch)  # Forward pass through the GNN\n",
        "        gnn_features_list.append(gnn_features.cpu().numpy())  # Store the features\n",
        "\n",
        "# Concatenate all the features\n",
        "gnn_features = np.concatenate(gnn_features_list, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model('GNN_MODEL/model_epoch_52.h5', compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.compile(optimizer='adam', loss=MeanSquaredError(), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHyP9h5pI5PG",
        "outputId": "0a9fd67f-61e0-46ad-da88-7c0b407ae214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 6.8435 - mae: 1.9776\n",
            "Epoch 1: saving model to GNN_MODEL/model_epoch_01.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1452s\u001b[0m 2s/step - loss: 6.8398 - mae: 1.9770 - val_loss: 7.8804 - val_mae: 2.3939\n",
            "Epoch 2/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 2.6622 - mae: 1.2631\n",
            "Epoch 2: saving model to GNN_MODEL/model_epoch_02.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1391s\u001b[0m 2s/step - loss: 2.6620 - mae: 1.2631 - val_loss: 5.1337 - val_mae: 1.9252\n",
            "Epoch 3/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 2.2719 - mae: 1.1483\n",
            "Epoch 3: saving model to GNN_MODEL/model_epoch_03.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1464s\u001b[0m 2s/step - loss: 2.2718 - mae: 1.1482 - val_loss: 3.0808 - val_mae: 1.3575\n",
            "Epoch 4/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 2.1302 - mae: 1.1114\n",
            "Epoch 4: saving model to GNN_MODEL/model_epoch_04.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1397s\u001b[0m 2s/step - loss: 2.1302 - mae: 1.1114 - val_loss: 2.8568 - val_mae: 1.2940\n",
            "Epoch 5/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 2.0259 - mae: 1.0764\n",
            "Epoch 5: saving model to GNN_MODEL/model_epoch_05.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1496s\u001b[0m 2s/step - loss: 2.0259 - mae: 1.0764 - val_loss: 3.7630 - val_mae: 1.5870\n",
            "Epoch 6/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.9501 - mae: 1.0527\n",
            "Epoch 6: saving model to GNN_MODEL/model_epoch_06.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1380s\u001b[0m 2s/step - loss: 1.9501 - mae: 1.0527 - val_loss: 2.4473 - val_mae: 1.1591\n",
            "Epoch 7/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.9044 - mae: 1.0400\n",
            "Epoch 7: saving model to GNN_MODEL/model_epoch_07.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1323s\u001b[0m 2s/step - loss: 1.9043 - mae: 1.0400 - val_loss: 1.9143 - val_mae: 0.9792\n",
            "Epoch 8/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.8318 - mae: 1.0193\n",
            "Epoch 8: saving model to GNN_MODEL/model_epoch_08.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1399s\u001b[0m 2s/step - loss: 1.8318 - mae: 1.0193 - val_loss: 1.6358 - val_mae: 0.9298\n",
            "Epoch 9/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.8184 - mae: 1.0144\n",
            "Epoch 9: saving model to GNN_MODEL/model_epoch_09.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1301s\u001b[0m 2s/step - loss: 1.8184 - mae: 1.0144 - val_loss: 1.8691 - val_mae: 0.9547\n",
            "Epoch 10/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.7848 - mae: 1.0015\n",
            "Epoch 10: saving model to GNN_MODEL/model_epoch_10.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1089s\u001b[0m 2s/step - loss: 1.7848 - mae: 1.0015 - val_loss: 1.6816 - val_mae: 0.9093\n",
            "Epoch 11/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.7377 - mae: 0.9820\n",
            "Epoch 11: saving model to GNN_MODEL/model_epoch_11.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1090s\u001b[0m 2s/step - loss: 1.7377 - mae: 0.9820 - val_loss: 2.0630 - val_mae: 1.0265\n",
            "Epoch 12/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.7101 - mae: 0.9789\n",
            "Epoch 12: saving model to GNN_MODEL/model_epoch_12.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1092s\u001b[0m 2s/step - loss: 1.7101 - mae: 0.9789 - val_loss: 1.5950 - val_mae: 0.9133\n",
            "Epoch 13/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.6821 - mae: 0.9730\n",
            "Epoch 13: saving model to GNN_MODEL/model_epoch_13.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1212s\u001b[0m 2s/step - loss: 1.6821 - mae: 0.9730 - val_loss: 1.9561 - val_mae: 0.9790\n",
            "Epoch 14/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.6622 - mae: 0.9607\n",
            "Epoch 14: saving model to GNN_MODEL/model_epoch_14.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1204s\u001b[0m 2s/step - loss: 1.6622 - mae: 0.9607 - val_loss: 1.5325 - val_mae: 0.8887\n",
            "Epoch 15/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.6118 - mae: 0.9483\n",
            "Epoch 15: saving model to GNN_MODEL/model_epoch_15.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1197s\u001b[0m 2s/step - loss: 1.6118 - mae: 0.9483 - val_loss: 1.9015 - val_mae: 0.9630\n",
            "Epoch 16/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.6208 - mae: 0.9471\n",
            "Epoch 16: saving model to GNN_MODEL/model_epoch_16.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1178s\u001b[0m 2s/step - loss: 1.6208 - mae: 0.9471 - val_loss: 1.4490 - val_mae: 0.8691\n",
            "Epoch 17/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.5451 - mae: 0.9264\n",
            "Epoch 17: saving model to GNN_MODEL/model_epoch_17.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1194s\u001b[0m 2s/step - loss: 1.5452 - mae: 0.9264 - val_loss: 1.5361 - val_mae: 0.8711\n",
            "Epoch 18/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.5693 - mae: 0.9298\n",
            "Epoch 18: saving model to GNN_MODEL/model_epoch_18.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1209s\u001b[0m 2s/step - loss: 1.5692 - mae: 0.9298 - val_loss: 1.4468 - val_mae: 0.9026\n",
            "Epoch 19/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.5274 - mae: 0.9198\n",
            "Epoch 19: saving model to GNN_MODEL/model_epoch_19.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1181s\u001b[0m 2s/step - loss: 1.5274 - mae: 0.9198 - val_loss: 1.4627 - val_mae: 0.8873\n",
            "Epoch 20/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.5618 - mae: 0.9294\n",
            "Epoch 20: saving model to GNN_MODEL/model_epoch_20.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1204s\u001b[0m 2s/step - loss: 1.5618 - mae: 0.9294 - val_loss: 1.5122 - val_mae: 0.8550\n",
            "Epoch 21/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.5053 - mae: 0.9076\n",
            "Epoch 21: saving model to GNN_MODEL/model_epoch_21.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1202s\u001b[0m 2s/step - loss: 1.5053 - mae: 0.9076 - val_loss: 1.5176 - val_mae: 0.8547\n",
            "Epoch 22/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.5375 - mae: 0.9210\n",
            "Epoch 22: saving model to GNN_MODEL/model_epoch_22.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1171s\u001b[0m 2s/step - loss: 1.5375 - mae: 0.9210 - val_loss: 1.4412 - val_mae: 0.8967\n",
            "Epoch 23/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.5035 - mae: 0.9118\n",
            "Epoch 23: saving model to GNN_MODEL/model_epoch_23.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1161s\u001b[0m 2s/step - loss: 1.5035 - mae: 0.9118 - val_loss: 1.4414 - val_mae: 0.8763\n",
            "Epoch 24/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4695 - mae: 0.8983\n",
            "Epoch 24: saving model to GNN_MODEL/model_epoch_24.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1199s\u001b[0m 2s/step - loss: 1.4695 - mae: 0.8983 - val_loss: 1.4571 - val_mae: 0.8427\n",
            "Epoch 25/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4698 - mae: 0.8976\n",
            "Epoch 25: saving model to GNN_MODEL/model_epoch_25.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1216s\u001b[0m 2s/step - loss: 1.4698 - mae: 0.8976 - val_loss: 1.4484 - val_mae: 0.8819\n",
            "Epoch 26/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4521 - mae: 0.8936\n",
            "Epoch 26: saving model to GNN_MODEL/model_epoch_26.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1241s\u001b[0m 2s/step - loss: 1.4521 - mae: 0.8936 - val_loss: 1.4235 - val_mae: 0.8743\n",
            "Epoch 27/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4636 - mae: 0.8928\n",
            "Epoch 27: saving model to GNN_MODEL/model_epoch_27.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1221s\u001b[0m 2s/step - loss: 1.4636 - mae: 0.8928 - val_loss: 1.4360 - val_mae: 0.8671\n",
            "Epoch 28/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4384 - mae: 0.8884\n",
            "Epoch 28: saving model to GNN_MODEL/model_epoch_28.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1231s\u001b[0m 2s/step - loss: 1.4384 - mae: 0.8884 - val_loss: 1.3963 - val_mae: 0.8739\n",
            "Epoch 29/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3997 - mae: 0.8715\n",
            "Epoch 29: saving model to GNN_MODEL/model_epoch_29.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1218s\u001b[0m 2s/step - loss: 1.3997 - mae: 0.8715 - val_loss: 1.4468 - val_mae: 0.8385\n",
            "Epoch 30/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4174 - mae: 0.8803\n",
            "Epoch 30: saving model to GNN_MODEL/model_epoch_30.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1215s\u001b[0m 2s/step - loss: 1.4174 - mae: 0.8803 - val_loss: 1.4581 - val_mae: 0.9247\n",
            "Epoch 31/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4073 - mae: 0.8795\n",
            "Epoch 31: saving model to GNN_MODEL/model_epoch_31.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1250s\u001b[0m 2s/step - loss: 1.4074 - mae: 0.8795 - val_loss: 1.3905 - val_mae: 0.8694\n",
            "Epoch 32/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4192 - mae: 0.8832\n",
            "Epoch 32: saving model to GNN_MODEL/model_epoch_32.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1191s\u001b[0m 2s/step - loss: 1.4191 - mae: 0.8832 - val_loss: 1.4173 - val_mae: 0.8945\n",
            "Epoch 33/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4267 - mae: 0.8784\n",
            "Epoch 33: saving model to GNN_MODEL/model_epoch_33.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1212s\u001b[0m 2s/step - loss: 1.4267 - mae: 0.8783 - val_loss: 1.4137 - val_mae: 0.8466\n",
            "Epoch 34/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3915 - mae: 0.8687\n",
            "Epoch 34: saving model to GNN_MODEL/model_epoch_34.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1241s\u001b[0m 2s/step - loss: 1.3915 - mae: 0.8687 - val_loss: 1.4088 - val_mae: 0.8422\n",
            "Epoch 35/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3639 - mae: 0.8608\n",
            "Epoch 35: saving model to GNN_MODEL/model_epoch_35.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1233s\u001b[0m 2s/step - loss: 1.3640 - mae: 0.8608 - val_loss: 1.3726 - val_mae: 0.8247\n",
            "Epoch 36/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3604 - mae: 0.8550\n",
            "Epoch 36: saving model to GNN_MODEL/model_epoch_36.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1238s\u001b[0m 2s/step - loss: 1.3604 - mae: 0.8550 - val_loss: 1.4409 - val_mae: 0.8397\n",
            "Epoch 37/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3659 - mae: 0.8593\n",
            "Epoch 37: saving model to GNN_MODEL/model_epoch_37.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1204s\u001b[0m 2s/step - loss: 1.3659 - mae: 0.8593 - val_loss: 1.3793 - val_mae: 0.8673\n",
            "Epoch 38/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3595 - mae: 0.8580\n",
            "Epoch 38: saving model to GNN_MODEL/model_epoch_38.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1209s\u001b[0m 2s/step - loss: 1.3595 - mae: 0.8580 - val_loss: 1.4397 - val_mae: 0.9195\n",
            "Epoch 39/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3651 - mae: 0.8621\n",
            "Epoch 39: saving model to GNN_MODEL/model_epoch_39.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1227s\u001b[0m 2s/step - loss: 1.3651 - mae: 0.8621 - val_loss: 1.4342 - val_mae: 0.8471\n",
            "Epoch 40/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3498 - mae: 0.8530\n",
            "Epoch 40: saving model to GNN_MODEL/model_epoch_40.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1236s\u001b[0m 2s/step - loss: 1.3497 - mae: 0.8530 - val_loss: 1.3674 - val_mae: 0.8142\n",
            "Epoch 41/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3366 - mae: 0.8448\n",
            "Epoch 41: saving model to GNN_MODEL/model_epoch_41.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1208s\u001b[0m 2s/step - loss: 1.3367 - mae: 0.8448 - val_loss: 1.5239 - val_mae: 0.8659\n",
            "Epoch 42/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3776 - mae: 0.8658\n",
            "Epoch 42: saving model to GNN_MODEL/model_epoch_42.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1226s\u001b[0m 2s/step - loss: 1.3776 - mae: 0.8658 - val_loss: 1.3832 - val_mae: 0.8265\n",
            "Epoch 43/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3094 - mae: 0.8351\n",
            "Epoch 43: saving model to GNN_MODEL/model_epoch_43.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1248s\u001b[0m 2s/step - loss: 1.3094 - mae: 0.8351 - val_loss: 1.3646 - val_mae: 0.8354\n",
            "Epoch 44/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3051 - mae: 0.8330\n",
            "Epoch 44: saving model to GNN_MODEL/model_epoch_44.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1247s\u001b[0m 2s/step - loss: 1.3052 - mae: 0.8330 - val_loss: 1.3430 - val_mae: 0.8141\n",
            "Epoch 45/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3062 - mae: 0.8315\n",
            "Epoch 45: saving model to GNN_MODEL/model_epoch_45.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1215s\u001b[0m 2s/step - loss: 1.3063 - mae: 0.8315 - val_loss: 1.3568 - val_mae: 0.8551\n",
            "Epoch 46/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3174 - mae: 0.8392\n",
            "Epoch 46: saving model to GNN_MODEL/model_epoch_46.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1243s\u001b[0m 2s/step - loss: 1.3174 - mae: 0.8392 - val_loss: 1.3635 - val_mae: 0.8507\n",
            "Epoch 47/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.3056 - mae: 0.8368\n",
            "Epoch 47: saving model to GNN_MODEL/model_epoch_47.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1259s\u001b[0m 2s/step - loss: 1.3057 - mae: 0.8368 - val_loss: 1.3617 - val_mae: 0.8170\n",
            "Epoch 48/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.2959 - mae: 0.8289\n",
            "Epoch 48: saving model to GNN_MODEL/model_epoch_48.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1281s\u001b[0m 2s/step - loss: 1.2959 - mae: 0.8289 - val_loss: 1.3269 - val_mae: 0.8061\n",
            "Epoch 49/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.2901 - mae: 0.8264\n",
            "Epoch 49: saving model to GNN_MODEL/model_epoch_49.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 2s/step - loss: 1.2902 - mae: 0.8264 - val_loss: 1.3749 - val_mae: 0.8793\n",
            "Epoch 50/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.2784 - mae: 0.8225\n",
            "Epoch 50: saving model to GNN_MODEL/model_epoch_50.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1296s\u001b[0m 2s/step - loss: 1.2784 - mae: 0.8225 - val_loss: 1.3369 - val_mae: 0.8143\n",
            "Epoch 51/100\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.2837 - mae: 0.8201\n",
            "Epoch 51: saving model to GNN_MODEL/model_epoch_51.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1270s\u001b[0m 2s/step - loss: 1.2837 - mae: 0.8201 - val_loss: 1.3217 - val_mae: 0.8112\n",
            "Epoch 52/100\n",
            "\u001b[1m276/705\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m12:34\u001b[0m 2s/step - loss: 1.3349 - mae: 0.8418"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the combined model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_protein\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgnn_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_kd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.18\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_cb\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NongNam\\Documents\\AI_Builder\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Train the combined model\n",
        "history = model.fit(\n",
        "    [X_protein, gnn_features],\n",
        "    y_kd,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    validation_split=0.18,\n",
        "    callbacks=[checkpoint_cb, history_cb]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'glob' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load all history files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history_files = \u001b[38;5;28msorted\u001b[39m(\u001b[43mglob\u001b[49m.glob(\u001b[33m'\u001b[39m\u001b[33mGNN_MODEL/history_epoch_*.pkl\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      4\u001b[39m full_history = {}\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m history_files:\n",
            "\u001b[31mNameError\u001b[39m: name 'glob' is not defined"
          ]
        }
      ],
      "source": [
        "# Load all history files\n",
        "history_files = sorted(glob.glob('GNN_MODEL/history_epoch_*.pkl'))\n",
        "\n",
        "full_history = {}\n",
        "for file in history_files:\n",
        "    with open(file, 'rb') as f:\n",
        "        epoch_history = pickle.load(f)\n",
        "    for key, values in epoch_history.items():\n",
        "        full_history.setdefault(key, []).extend(values[len(full_history.get(key, [])):])\n",
        "\n",
        "# Plot\n",
        "\n",
        "\n",
        "plt.plot(full_history['val_loss'])\n",
        "plt.plot(full_history['loss'])\n",
        "plt.title('Model accuracy over all epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
